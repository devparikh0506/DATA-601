{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEEK 9: Removing punctuation and stop words from a corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing ntlk and python-docx libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\devdp\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\devdp\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\devdp\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\devdp\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\devdp\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\devdp\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: python-docx in c:\\users\\devdp\\anaconda3\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\devdp\\anaconda3\\lib\\site-packages (from python-docx) (4.9.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\devdp\\anaconda3\\lib\\site-packages (from python-docx) (4.9.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install python-docx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from zipfile import ZipFile\n",
    "import glob\n",
    "import os\n",
    "import string\n",
    "import re\n",
    "from docx import Document\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now downloading stopwords and punkt modules from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\devdp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\devdp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filenames():\n",
    "    \"\"\"\n",
    "    Extracts the zip file containig all text files\n",
    "    then returns list of filenames of all files in zip directory\n",
    "\n",
    "    Returns:\n",
    "        list[str]: list of filenames\n",
    "    \"\"\"\n",
    "    with ZipFile('week_10_txt_and_docx.zip', 'r') as zipDir:\n",
    "        path = r'week_10_txt_and_docx'\n",
    "        zipDir.extractall('week_10_txt_and_docx')\n",
    "        filenames = glob.glob(path + \"/*.txt\") + glob.glob(path + \"/*.docx\") # concating .text and .docx files\n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week_10_txt_and_docx\\52256-0.txt\n",
      "week_10_txt_and_docx\\53031-0.txt\n",
      "week_10_txt_and_docx\\58108-0.txt\n",
      "week_10_txt_and_docx\\blind_text.txt\n",
      "week_10_txt_and_docx\\dr_yawn.txt\n",
      "week_10_txt_and_docx\\how_rubber_goods_are_made.txt\n",
      "week_10_txt_and_docx\\most_boring_ever.txt\n",
      "week_10_txt_and_docx\\most_boring_part2.txt\n",
      "week_10_txt_and_docx\\pg12814.txt\n",
      "week_10_txt_and_docx\\pg14895.txt\n",
      "week_10_txt_and_docx\\pg43994.txt\n",
      "week_10_txt_and_docx\\random_text.txt\n",
      "week_10_txt_and_docx\\smiley_the_bunny.txt\n",
      "week_10_txt_and_docx\\week_10_document1.docx\n",
      "week_10_txt_and_docx\\week_10_document2.docx\n"
     ]
    }
   ],
   "source": [
    "filenames = get_filenames()\n",
    "print('\\n'.join(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_punctuation_and_stop_words(filenames: list[str]):\n",
    "    \"\"\"\n",
    "    This function reads all files and removes punctuation and stopwords.\n",
    "    It uses ntlk to tokenize and remove stopwords from sentence.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        filenames (list[str]): list of filenames\n",
    "\n",
    "    Returns:\n",
    "        dict: dictioary with key as filename and value as list of unique words\n",
    "    \"\"\"\n",
    "    stop_words = set(map(lambda x: x.lower(), list(\n",
    "        set(stopwords.words('english')))))\n",
    "    # https://www.geeksforgeeks.org/python-check-url-string/\n",
    "    url_regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "\n",
    "    # https://stackoverflow.com/a/3868861\n",
    "    phone_regex = r'(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})'\n",
    "\n",
    "    words_data = dict()\n",
    "    for filename in filenames:\n",
    "        with open(filename, 'r') as file:\n",
    "            file_text = None\n",
    "            try:\n",
    "                file_text = file.read()\n",
    "\n",
    "            except UnicodeDecodeError:  # Handleling file decoding errors\n",
    "                if '.docx' in filename:\n",
    "                    # if file is a docx file then using python-docx to load file\n",
    "                    # ref : https://python-docx.readthedocs.io/en/latest/user/documents.html#opening-a-file-like-document\n",
    "                    file = open(filename, 'rb')\n",
    "                    document = Document(file)\n",
    "                    file_text = '\\n'.join(\n",
    "                        list(map(lambda x: x.text, [paragrphs for paragrphs in document.paragraphs])))\n",
    "                    file.close()\n",
    "                else:\n",
    "                    print(\"Error decodingin file : %s\" % filename)\n",
    "                    continue\n",
    "\n",
    "            # Check if file_text if not None i.e. file is loaded successfully.\n",
    "            if file_text is not None:\n",
    "\n",
    "                # removes urls from text\n",
    "                file_text = re.sub(url_regex, \" \", file_text)\n",
    "\n",
    "                # removes phone numbers from text\n",
    "                file_text = re.sub(phone_regex, ' ', file_text)\n",
    "                words = []\n",
    "\n",
    "                # this will detect non-alphanumeric characters and this will also identify \"_\".\n",
    "                text_only_regex = r'[^\\w\\s]|_+'\n",
    "                for sentence in list(set(sent_tokenize(file_text))):\n",
    "                    words += [\n",
    "                        word for word in\n",
    "                        word_tokenize(\n",
    "                            # replacing non-alpha-numeric chars with space\n",
    "                            re.sub(text_only_regex, ' ', sentence)\n",
    "                        )\n",
    "                        # checking if word is not a stopword\n",
    "                        if (word.lower() not in stop_words)\n",
    "                        and (word not in words)  # checking if word is unique\n",
    "                        # checking if word is not a punctuation\n",
    "                        and (word not in string.punctuation)\n",
    "                    ]\n",
    "                words_data[os.path.basename(filename)] = list(\n",
    "                    set(map(lambda x: x.lower(), words)))\n",
    "\n",
    "    return words_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_data = remove_punctuation_and_stop_words(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_words_data(words_data):\n",
    "    \"\"\"\n",
    "    Generates .dat file from data  \n",
    "    \"\"\"\n",
    "    data = ''\n",
    "    for key, value in words_data.items():\n",
    "        # logic to convert data into target format string\n",
    "        data += \"\"\"\"%s\":%s\"\"\" % (key, ','.join(value))\n",
    "\n",
    "    with open('output.dat', 'w') as outputfile:\n",
    "        outputfile.write(data)\n",
    "    return \"\"\"Generated \"output.dat\" file\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Generated \"output.dat\" file'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_words_data(words_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
